<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Virtual Assistant</title>
  <style>
    body {
      background: #0f172a;
      color: white;
      font-family: sans-serif;
      text-align: center;
      padding: 2rem;
    }

    #mic-button {
      background: #3b82f6;
      border: none;
      padding: 1rem 2rem;
      font-size: 1.2rem;
      color: white;
      border-radius: 999px;
      cursor: pointer;
      margin-bottom: 1rem;
    }

    canvas {
      display: block;
      margin: 1rem auto;
      background: #1e293b;
      border-radius: 8px;
    }

    #transcript {
      margin-top: 1rem;
      font-size: 1.2rem;
      color: #a5f3fc;
    }
  </style>
</head>
<body>
  <h1>üéôÔ∏è Virtual Assistant</h1>
  <button id="mic-button">Start Listening</button>
  <canvas id="waveform" width="600" height="100"></canvas>
  <div id="transcript">Transcript: <span></span></div>

  <script>
    const micButton = document.getElementById('mic-button');
    const canvas = document.getElementById('waveform');
    const ctx = canvas.getContext('2d');
    const transcriptEl = document.querySelector('#transcript span');

    let listening = false;
    let audioContext;
    let analyser;
    let microphone;
    let dataArray;
    let recognition;

    function drawWaveform() {
      if (!analyser) return;

      requestAnimationFrame(drawWaveform);
      analyser.getByteTimeDomainData(dataArray);

      ctx.fillStyle = '#1e293b';
      ctx.fillRect(0, 0, canvas.width, canvas.height);
      ctx.lineWidth = 2;
      ctx.strokeStyle = '#38bdf8';
      ctx.beginPath();

      const sliceWidth = canvas.width / dataArray.length;
      let x = 0;

      for (let i = 0; i < dataArray.length; i++) {
        const v = dataArray[i] / 128.0;
        const y = (v * canvas.height) / 2;

        if (i === 0) ctx.moveTo(x, y);
        else ctx.lineTo(x, y);

        x += sliceWidth;
      }

      ctx.lineTo(canvas.width, canvas.height / 2);
      ctx.stroke();
    }

    async function startListening() {
      // Set up audio context for waveform
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      microphone = audioContext.createMediaStreamSource(stream);
      analyser = audioContext.createAnalyser();
      analyser.fftSize = 2048;
      dataArray = new Uint8Array(analyser.fftSize);
      microphone.connect(analyser);
      drawWaveform();

      // Speech recognition
      recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
      console.log("recognition", recognition)
            recognition.interimResults = true;
      recognition.continuous = true;
      recognition.onresult = (event) => {
        let transcript = '';
        for (const result of event.results) {
          transcript += result[0].transcript;
        }
        transcriptEl.textContent = transcript;
      };
      recognition.start();
    }

    function stopListening() {
      if (recognition) recognition.stop();
      if (audioContext) audioContext.close();
      ctx.clearRect(0, 0, canvas.width, canvas.height);
    }

    micButton.addEventListener('click', async () => {
      listening = !listening;
      micButton.textContent = listening ? 'Stop Listening' : 'Start Listening';
      transcriptEl.textContent = '';

      if (listening) {
        await startListening();
      } else {
        stopListening();
      }
    });

    if (recognition) {
        recognition.onresult = (event) => {
      let transcript = '';
      for (const result of event.results) {
        transcript += result[0].transcript;
      }
      transcriptEl.textContent = transcript;
    
      fetch('/ask', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text: transcript })
      })
      .then(res => res.json())
      .then(data => {
        transcriptEl.textContent += "\nAssistant: " + data.reply;
    
        // Play audio response
        const audio = new Audio(`/static/audio/${data.audio_file}`);
        audio.play();
      });
    };
    }
  </script>
</body>
</html>